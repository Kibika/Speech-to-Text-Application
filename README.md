# speech-to-text
This aim of this project is to build a model that can take in speech data as input and store the data in form of text. The dataset is provided in form of .wav files and their transcriptions in .txt. The data is saved in dvc to allow for versioning.
The first step is to preprocess the data and put it in a format that can be trained to give good results. The result of the preprocessing stage is an image of the data with dimensions that can be fed into a deep learning model. The transcriptiond are also mapped to integer values and later decoded to the original words after training. The other steps involved during preprocessing are cleaning of the audio and texts, choosing either a mono or stereo format, truncating the arrays and padding them to ensure they all have the same length and generating additional data through augmentation.
The next step is to train a deep learning model, 5 models were trained here but the bidirectional reccurent neural network was chose as the best model from the loss curves and taking into account that speech data gives access to 'future' data that is useful for prediction. After training we reproduce the audio, it's transcription and the transcription predicted by the model. The word error rate is also compute to evaluate the model. 
The final model chosen is deployed as an application that allows individual to upload a soundfile and get back it's transcription.
